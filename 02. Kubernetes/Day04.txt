Task: https://youtu.be/3fhvnsNY5fc?si=U6gX5T9xMQrfMt_P

Ingress Controller
=================
Home Page - 81
Products Page - 82
Cart Page - 83
....

Port Based Routing
PublicIP:82

Path Based Routing

----------------------------------------------------------------------
INGRESS CONTROLLER DEMO
----------------------------------------------------------------------
Install Docker
apt install docker.io

Install tree
apt install tree -y

Install ingress controller ---->
To install ingress, firstly we have to install nginx ingress controller:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/cloud/deploy.yaml

kubectl get ing --> shows ingress service , no ingress service

Verify the ingress pods
kubectl get pods -n ingress-nginx

Verify the ingress service 
kubectl get svc -n ingress-nginx ----> You will see load balancer url
aaf62657d0dc04b35be250af4d6e47f7-1092281608.ap-south-1.elb.amazonaws.com
We will see the above URL to access the app

Create the directory structure as below;
.
â”œâ”€â”€ app1
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ index.html
â”œâ”€â”€ app2
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ index.html
â”œâ”€â”€ app3
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ index.html
â””â”€â”€ k8s
    â”œâ”€â”€ app1-deploy.yaml
    â”œâ”€â”€ app2-deploy.yaml
    â”œâ”€â”€ app3-deploy.yaml
    â””â”€â”€ ingress.yaml

Dockerfile is same in all the directories;
FROM nginx:alpine
COPY ./index.html /usr/share/nginx/html/index.html

app1/index.html file content ---->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>App 1 - Ingress Demo</title>
  <style>
    body {
      margin: 0;
      background: linear-gradient(135deg, #ff4e50, #f9d423);
      font-family: 'Segoe UI', sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      color: #fff;
      text-align: center;
      animation: fadeIn 1s ease-in;
    }
    h1 {
      font-size: 4rem;
      margin-bottom: 10px;
      text-shadow: 2px 2px 8px rgba(0,0,0,0.3);
    }
    p {
      font-size: 1.5rem;
      max-width: 600px;
      line-height: 1.5;
    }
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(-10px); }
      to { opacity: 1; transform: translateY(0); }
    }
  </style>
</head>
<body>
  <div>
    <h1>ðŸš€ App 1 Loaded</h1>
    <p>This is the futuristic red version of App 1, powered by Kubernetes & Ingress on AWS EKS.</p>
  </div>
</body>
</html>


app2/index.html file content ---->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>App 2 - Ingress Demo</title>
  <style>
    body {
      margin: 0;
      background: linear-gradient(135deg, #56ab2f, #a8e063);
      font-family: 'Helvetica Neue', sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      color: #fff;
      text-align: center;
      animation: slideUp 1s ease-in-out;
    }
    h1 {
      font-size: 3.5rem;
      margin-bottom: 15px;
    }
    p {
      font-size: 1.4rem;
    }
    @keyframes slideUp {
      from { opacity: 0; transform: translateY(20px); }
      to { opacity: 1; transform: translateY(0); }
    }
  </style>
</head>
<body>
  <div>
    <h1>ðŸŒ¿ App 2 is Live</h1>
    <p>This is App 2, deployed in Kubernetes and beautifully routed via Ingress.</p>
  </div>
</body>
</html>


app3/index.html file content ---->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>App 3 - Ingress Demo</title>
  <style>
    body {
      margin: 0;
      background: linear-gradient(135deg, #1e3c72, #2a5298);
      font-family: 'Roboto', sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      color: #ffffffcc;
      text-align: center;
      animation: zoomIn 1s ease;
    }
    h1 {
      font-size: 4rem;
      margin-bottom: 10px;
    }
    p {
      font-size: 1.3rem;
    }
    @keyframes zoomIn {
      from { opacity: 0; transform: scale(0.95); }
      to { opacity: 1; transform: scale(1); }
    }
  </style>
</head>
<body>
  <div>
    <h1>ðŸŒŠ Welcome to App 3</h1>
    <p>This is App 3 â€” clean, responsive, and routed with Kubernetes Ingress on EKS.</p>
  </div>
</body>
</html>
==================================
2048 Game with SSL Certificate

aws eks update-kubeconfig --region <ClusterRegion> --name <ClusterName>

Set your cluster name and region in an environment variable:
cluster_name=<ClusterName>
export AWS_REGION=<ClusterRegion>

Extract the OIDC ID from your cluster:
oidc_id=$(aws eks describe-cluster --name $cluster_name --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
echo $oidc_id

Check if an IAM OIDC provider already exists for your cluster:
aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4
If output is returned: You already have an IAM OIDC provider and can skip the next step.

If no output is returned: Create an IAM OIDC provider for your cluster:
eksctl utils associate-iam-oidc-provider --cluster kastro-cluster --approve --region us-east-1
aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4
You will see the output, the same OIDC provider will also be there in EKS Console
Lets verify;
EKS -----> Open the Cluster ----> Under "Overview" you will see "OpenID Connect provider URL" ----> You can check the OIDC 

Lets configure aws load balancer controller;
Download the IAM policy required for the AWS Load Balancer Controller:
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json
ls
You will see a file called "iam_policy.json"

Create the IAM policy:
aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy.json

The name of the policy which we have created is "AWSLoadBalancerControllerIAMPolicy"
The same policy will be created in IAM console. Goto IAM console and verify. Copy the ARN of the policy created.
Ex: "Arn": "arn:aws:iam::560185625463:policy/AWSLoadBalancerControllerIAMPolicy"

3.2 â€“ Create the IAM Service Account
Create the IAM service account for the AWS Load Balancer Controller using eksctl. This command attaches the policy to the service account and (if it exists) overrides the existing service account:

eksctl create iamserviceaccount \
    --cluster=<ClusterName> \
    --namespace=kube-system \
    --name=aws-load-balancer-controller \
    --attach-policy-arn=<ARNofThePolicy> \
    --override-existing-serviceaccounts \
    --region <ClusterRegion> \
    --approve

Ex:
eksctl create iamserviceaccount \
    --cluster=kastro-cluster \
    --namespace=kube-system \
    --name=aws-load-balancer-controller \
    --attach-policy-arn=arn:aws:iam::560185625463:policy/AWSLoadBalancerControllerIAMPolicy \
    --override-existing-serviceaccounts \
    --region ap-south-1 \
    --approve

To verify;
kubectl get sa -n kube-system
You will see "aws-load-balancer-controller" got created

3.3 â€“ Install the AWS Load Balancer Controller using HELM
#Install HELM
curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version

Add the Helm repository and update it:
helm repo add eks https://aws.github.io/eks-charts
helm repo update eks

Install the AWS Load Balancer Controller:
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=kastro-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller

3.4 â€“ Verify the Controller Deployment
Confirm that the controller is installed and running:
kubectl get deployment -n kube-system aws-load-balancer-controller

Step 4: Deploy the 2048 Game Application
Below are the four YAML files provided. These files create a dedicated namespace, deploy the 2048 game, create a service, and set up an ingress to expose the application via the ALB.

4.1 â€“ Namespace (namespace.yaml)
apiVersion: v1
kind: Namespace
metadata:
  name: 2048-game

Apply the namespace:
kubectl apply -f namespace.yaml

4.2 â€“ Deployment (deployment.yaml)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: game-2048-deployment
  namespace: 2048-game
  labels:
    app: game-2048
spec:
  replicas: 2
  selector:
    matchLabels:
      app: game-2048
  template:
    metadata:
      labels:
        app: game-2048
    spec:
      containers:
      - name: game-2048
        image: thipparthiavinash/2048-game
        ports:
        - containerPort: 80

Deploy the application:
kubectl apply -f deployment.yaml

4.3 â€“ Service (service.yaml)
apiVersion: v1
kind: Service
metadata:
  name: game-2048-service
  namespace: 2048-game
  labels:
    app: game-2048
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    app: game-2048

Create the service:
kubectl apply -f service.yaml

4.4 â€“ Ingress (ingress.yaml)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: game-2048-ingress
  namespace: 2048-game
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}]'
spec:
  rules:
  - http:
      paths:
      - path: /*
        pathType: ImplementationSpecific
        backend:
          service:
            name: game-2048-service
            port:
              number: 80

4.5 Updated ingress yml
vi ingress-https.yml file ---->
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: game-2048-ingress
  namespace: 2048-game
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: <Your-Cert-ARN-From-ACM-Console>
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS13-1-2-2021-06
spec:
  rules:
  - host: game.learnaws.today
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: game-2048-service
            port:
              number: 80
=================================
Autoscaling in K8s;
Horizontal Pod Autoscaling
Vertical Pod Autoscaling

Types of autoscaling;
Memory based scaling
CPU based scaling

CPU > 80% ---> Increase the pod count
CPU < 10% ----> Decrease the pod count

Install Metrics Server to perform the autoscaling
If you are working with cloud specific clusters, metrics server will get installed by default



HPA

vi cpu-php-apache.yaml ---->

apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  labels:
    run: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache


vi cpu-hpa.yml ----> (CPU Based Scaling)

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 10


kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"


vi mem-php-apache.yaml ---->

apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  replicas: 1
  selector:
    matchLabels:
      run: php-apache
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 200m
            memory: 100Mi	#Guaranteed memory per pod
          limits:
            cpu: 500m
            memory: 200Mi	#Max memory pod can use. Incase of >200Mi, Pod is OOMKilled
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
spec:
  type: ClusterIP
  selector:
    run: php-apache
  ports:
  - port: 80
    targetPort: 80

vi mem-hpa.yml ----> (MEMORY Based Scaling)

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache-hpa-memory
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 10Mi

If average memory usage per pod exceeds 10Mi, HPA starts scaling

Lets generate load;
kubectl run -i --tty load-generator \
  --rm \
  --restart=Never \
  --image=busybox:1.28 \
  -- /bin/sh -c "while true; do wget -q -O- http://php-apache; done"

====================
ARGO CD
Any deployments that we do in Kubernetes has to be automated using GitOps Approach - ArgoCD
Jenkins ----- CI (clone the code, package the code, build the image, tag the image, push the image)
ArgoCD ---- CD (monitor the changes in the k8s manifest files, argocd will understand the changes in the ymls and then deploys the app accordingly)







Lets setup ArgoCD using HELM;
------------------------------------------------
Install HELM
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version

Install ARGOCD using HELM
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

kubectl create namespace argocd ----> 
Lets install argocd in the namespace 'argocd' ----> helm install argocd argo/argo-cd --namespace argocd ----> kubectl get all -n argocd ----> You will see multiple things which are running ----> Under 'services' you can see 'argo-cd server' and the type as ClusterIP. But to access outside of the cluster, we need Load Balancer. So lets edit this ClusterIP to LoadBalancer ----> For this i will use patch ----> 

EXPOSE ARGOCD SERVER:
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}' ----> kubectl get all -n argocd ----> Now you can see the service called 'argo-cd server' changed to Load Balancer instead of ClusterIP ----> Copy the load balancer url ----> 

yum install jq -y

kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'

TO GET ARGO CD PASSWORD:
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
 